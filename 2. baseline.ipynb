{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model Train"
      ],
      "metadata": {
        "id": "g7YV6HZoWpTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/images/'\n",
        "\n",
        "train_paths, test_paths = [], []\n",
        "\n",
        "for filename in train_df.image:\n",
        "  train_paths.append(main_path + filename + '.jpg')\n",
        "\n",
        "for filename in test_df.image:\n",
        "  test_paths.append(main_path + filename + '.jpg')\n",
        "\n",
        "train_paths = np.array(train_paths)\n",
        "test_paths = np.array(test_paths)\n",
        "\n",
        "train_labels = train_df.label.values\n",
        "test_labels = test_df.label.values\n",
        "\n",
        "len(train_paths), len(train_labels), len(test_paths), len(test_labels)\n",
        "######################################################################################################################################\n",
        "s = np.arange(len(train_paths))\n",
        "np.random.shuffle(s)\n",
        "\n",
        "train_paths = train_paths[s]\n",
        "train_labels = train_labels[s]\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_paths, train_labels, test_size=0.2, random_state=42)\n",
        "#######################################################################################################################################\n",
        "\n",
        "ss = s = np.arange(len(test_paths))\n",
        "np.random.shuffle(ss)\n",
        "\n",
        "test_paths = test_paths[ss]\n",
        "test_labels = test_labels[ss]\n",
        "\n",
        "def preprocessing(path, label):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.io.decode_jpeg(img)\n",
        "  img = tf.image.resize(img, (256,256))\n",
        "  img = img/255\n",
        "  return img, label\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(len(train_paths))\n",
        "train_dataset = train_dataset.map(preprocessing).batch(batch_size).prefetch(1)\n",
        "val_dataset = val_dataset.map(preprocessing).batch(batch_size).prefetch(1)\n",
        "test_dataset = test_dataset.map(preprocessing).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "9fJ5mh3bWgU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Conv2D(128,3,padding='same', input_shape=[256,256,3]))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "model.add(layers.Conv2D(128,3,padding='same'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "model.add(layers.Conv2D(256,3,padding='same'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "model.add(layers.Conv2D(512,3,padding='same'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(7, activation='softmax'))\n",
        "\n",
        "opt = keras.optimizers.SGD()\n",
        "# opt = keras.optimizers.SGD()\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=40, verbose=1, callbacks=[es])"
      ],
      "metadata": {
        "id": "pyxkzSPvWgSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "U1y66owaWuHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_preprocessing(path, label):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.io.decode_jpeg(img)\n",
        "  img = tf.image.resize(img, (256,256))\n",
        "  img = img/255\n",
        "  return img\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "pred_test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "pred_test_dataset = pred_test_dataset.map(pred_preprocessing).batch(batch_size).prefetch(1)\n",
        "\n",
        "y_pred = model.predict(pred_test_dataset)\n",
        "label_pred = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "g5DK9PdkWgP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(test_labels, label_pred, average='macro')\n",
        "recall = recall_score(test_labels, label_pred, average= \"macro\")\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "print(f'정밀도 : {precision:.4f}')\n",
        "print(f'재현율 : {recall:.4f}')\n",
        "print(f'f1_score : {F1:.4f}')"
      ],
      "metadata": {
        "id": "7nlH1XGXWyw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}